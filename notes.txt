Part A: Word Frequencies  (40 points)

Method/Function: List<Token> tokenize(TextFilePath)
Write a method/function that reads in a text file and returns a list of the tokens in that file. For the purposes of this project, a token is a sequence of alphanumeric characters, independent of capitalization (so Apple, apple, aPpLe are the same token). You are allowed to use regular expressions if you wish to (and you can use some regexp engine, no need to write it from scratch), but you are not allowed to import a tokenizer (e.g. from NLTK), since you are being asked to write a tokenizer.
Method:        Map<Token,Count> computeWordFrequencies(List<Token>)
Write another method/function that counts the number of occurrences of each token in the token list. Remember that you should write this assignment yourself from scratch so you are not allowed to import a counter when the assignment asks you to write that method.

Method:         void print(Frequencies<Token, Count>)
Finally, write a method that prints out the word frequency count onto the screen. The printout should be ordered by decreasing frequency (so, the highest frequency words first; if necessary, order the cases of ties alphabetically). 

List<Token> tokenize(TextFilePath)
def tokenize(str):
1. Read in text file
- go line by line
- everthing lowercase?
alphanumeric characters

2. look for valid tokens and add to list

3. Return list of tokens



Map<Token,Count> computeWordFrequencies(List<Token>)
def computeWordFrequencies(list[tokens])

1. read in token
dictionary! key:value
token:count 
string:int

2. if in dictionary, increment the count 1++
if the token is not already in the dictionary, add to dictionary and initialize count = 1

3. return dictionary



void print(Frequencies<Token, Count>)
ordered by decreasing frequency (so, the highest frequency words first; if necessary, order the cases of ties alphabetically).
sort by frequency, then alphabetically
1. sorted(dictionary, key= lambda x: [0])
use the operator module: to sort by frequency, then word
sorted(dictionary, key = itemgetter(1, 0), reverse=True, FALSE?)
**not sure how to makesure in decreasing orde rbut still alphabetical 

2. output:
<token> <freq>

PART A NOTES:
import sys
import re

file1 = open(sys.argv[1], 'r') 

while True:
    line = str(file1.readline())

    if not line:
        break
    print("The original line: ", line)
    # res = re.split(r'\W', line, flags=re.IGNORECASE | re.ASCII)
    # res = re.split(r'[ ]', line)
    res = re.findall(r'\w+', line)
    # line_tokens = re.findall(r'\w+', line)
    print("regex line: ", res)


file1.close()

PROBLEM? cant do alphabetical and descreasing a the same time, so had to do two separate sorts
def printTokenCounts(counts):
    """
        method that prints out the word frequency count onto the screen
        ordered by decreasing frequency and alphabetically)

        input:
        counts - dictionary with token(string):counts(int)

        return:
        prints the token and frequency with the format <token> <freq>
    """
    # OrderedDict makes sure the dictionary is ordered and printed in the right order
    sortedTokens = sorted(counts.items(), key = itemgetter(1, 0), reverse=True)
    print(sortedTokens)

PART A (with print statements):
import sys  # used to read the command line input
import re   # used to parse the file
from operator import itemgetter     # used to access values in dictionary
from collections import OrderedDict # guarantees keys stored in correct order

# function has linear time complexity O(n)
def tokenize(filepath):
    """
        function that reads in a text file and returns a list of the tokens in that file
        tokens are a sequence of alphanumeric characters, independent of capitalization

        input:
        filepath - path to the textfile
        return:
        tokens - list of tokens (lowercase)
    """
    # get and open the file from the command line argument
    file1 = open(filepath, 'r') 
    # instantiate tokens list that will be returned
    tokens = []

    # read in the file line by line
    # total time efficiency of while loop is linear
    # O(n + n) = O(2n) => O(n)
    while True:
        line = str(file1.readline())

        # stop loop when end of file is reached
        if not line:
            break
        # print("The original line: ", line)

        # use regex findall function to find the tokens in the current line 
        # while filtering the non-alphanumeric chatracters
        # linear time efficiency O(n)
        line_tokens = re.findall(r'[a-zA-Z0-9]+', line)
        # print("regex line: ", line_tokens)

        # append every token in the line to tokens list
        # linear time efficiency O(n)
        for token in line_tokens:
            tokens.append(token.lower())

    file1.close()

    # return the token list
    return tokens

# function has linear time complexity O(n)
def computeWordFrequencies(tokens):
    """
        function that counts the number of occurrences of each token in the token list
        and returns a dictionary with the counts for each token

        input:
        tokens - list of tokens

        return:
        counts - dictionary with token(string):count(int)
    """
    # initialize dictionary
    counts = {}

    # go through every token in the list
    # for loop has linear time efficiency O(n) since dictionaries 
    # have O(1) lookup and insertion properties
    for token in tokens:
        # if token is in dictionary, increment count value
        # O(1) constant time complexity when checking for value in dictionary
        if token in counts:
            counts[token] += 1
        else:
            # if token is not in dictionary, add new token with count = 1
            counts.update({token:1})
        print("the current count for {token} is {count}".format(token = token, count = counts[token]))
    
    return counts


# total time efficiency = O(nlogn + nlogn + n) = O(2nlogn + n) = O(nlogn)
# this comes from the use of python's sorted() method
def printTokenCounts(counts):
    """
        method that prints out the word frequency count onto the screen
        ordered by decreasing frequency and alphabetically)

        input:
        counts - dictionary with token(string):counts(int)

        return:
        prints the token and frequency with the format <token> <freq>
    """
    # OrderedDict makes sure the dictionary is ordered and printed in the right order
    # first sort the dictionary by the tokens alphabetically
    # Python's Timsort has worst case time complexity O(nlogn) and O(n) best case
    sortedTokens = OrderedDict(sorted(counts.items(), key=itemgetter(0)))
    print(sortedTokens)
    # next, sort the dictionary in decreasing frequency
    # time complexity O(nlogn)
    sortedCounts = OrderedDict(sorted(sortedTokens.items(), key = itemgetter(1), reverse=True))
    print(sortedCounts)
    
    # print each token and frequency pair
    # linear time efficiency O(n)
    for token, count in sortedCounts.items():
        print(token, count)

def main():
    tokens = tokenize(sys.argv[1])
    print(tokens)

    tokencounts = computeWordFrequencies(tokens)
    print(tokencounts)
    printTokenCounts(tokencounts)
    


if __name__ == "__main__":
    main()

-changed to only use one list when printing the dictionary


PART B:
Write a program that takes two text files from the command line as arguments and outputs the number of tokens they have in common. Here is an example of input/output:

You can reuse the code you wrote for part A (remember that you can import files, avoiding thus code duplication!).
The TA will use their own text files. Note that some of the text files may be VERY LARGE, so make sure that your program is not dependent on reading the entire files to the computer RAM. 
For this part, programs that perform better will be given more credit than those that perform poorly.

1. read in file1 
tokenize 
use computeWordFrequencies to put into a dictionary = file1tokens

2. intersection = empty set!! {}
num_common = 0

3. read in file2
tokenize
check if token in file1tokens 
True   
    -> check if in intersection set (linear time)
        False
            -> add to intersection set 
    increment num_common

4. print intersection
print num_common

